{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "from datasets import load_dataset\n",
        "from rouge_score import rouge_scorer\n",
        "from bert_score import score\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "# Adjust the root logger's level (which affects all libraries unless overridden)\n",
        "logging.basicConfig(level=logging.ERROR)\n",
        "\n",
        "# Set specific loggers for libraries that are too verbose\n",
        "datasets_logger = logging.getLogger(\"datasets\")\n",
        "transformers_logger = logging.getLogger(\"transformers\")\n",
        "datasets_logger.setLevel(logging.ERROR)\n",
        "transformers_logger.setLevel(logging.ERROR)\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model_path = 'final_model_summarization'\n",
        "model = BartForConditionalGeneration.from_pretrained(model_path)\n",
        "tokenizer = BartTokenizer.from_pretrained(model_path)\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"ccdv/cnn_dailymail\", \"3.0.0\", split='validation')\n",
        "sampled_dataset = dataset.shuffle(seed=42).select(range(int(len(dataset) * 0.1)))\n",
        "\n",
        "# Initialize ROUGE scorer\n",
        "rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "# Lists to hold scores and summaries\n",
        "rouge_scores = []\n",
        "bert_precisions = []\n",
        "bert_recalls = []\n",
        "bert_f1s = []\n",
        "\n",
        "# Function to generate summaries\n",
        "def generate_summary(article):\n",
        "    inputs = tokenizer(article, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "    summary_ids = model.generate(inputs[\"input_ids\"], max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "# Iterate over the sampled dataset\n",
        "for example in tqdm(sampled_dataset, desc=\"Processing dataset\"):\n",
        "    article = example['article']\n",
        "    highlight = example['highlights']\n",
        "    \n",
        "    # Generate summary\n",
        "    generated_summary = generate_summary(article)\n",
        "\n",
        "    # Compute ROUGE score\n",
        "    rouge_scores.append(rouge_scorer.score(generated_summary, highlight))\n",
        "    \n",
        "    # Compute BERTScore\n",
        "    P, R, F1 = score([generated_summary], [highlight], lang='en', verbose=False)\n",
        "    bert_precisions.append(P.numpy())\n",
        "    bert_recalls.append(R.numpy())\n",
        "    bert_f1s.append(F1.numpy())\n",
        "\n",
        "# Calculate average scores\n",
        "average_rouge = {key: np.mean([score[key].fmeasure for score in rouge_scores]) for key in ['rouge1', 'rouge2', 'rougeL']}\n",
        "average_bert_precision = np.mean(bert_precisions)\n",
        "average_bert_recall = np.mean(bert_recalls)\n",
        "average_bert_f1 = np.mean(bert_f1s)\n",
        "\n",
        "print(\"Average ROUGE scores:\", average_rouge)\n",
        "print(\"Average BERT Precision:\", average_bert_precision)\n",
        "print(\"Average BERT Recall:\", average_bert_recall)\n",
        "print(\"Average BERT F1:\", average_bert_f1)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "INFO:absl:Using default tokenizer.\nProcessing dataset: 100%|██████████| 1336/1336 [1:57:29<00:00,  5.28s/it] \n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Average ROUGE scores: {'rouge1': 0.43579883764465166, 'rouge2': 0.20855940463406772, 'rougeL': 0.30107523353961063}\nAverage BERT Precision: 0.88010025\nAverage BERT Recall: 0.87622803\nAverage BERT F1: 0.8780281\n"
        }
      ],
      "execution_count": 16,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1715110659644
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "nlp",
      "language": "python",
      "display_name": "nlp"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.19",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "nlp"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}