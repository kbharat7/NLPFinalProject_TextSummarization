{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Check if CUDA is available\n",
        "print(\"CUDA Available: \", torch.cuda.is_available())\n",
        "\n",
        "# Get the name of the CUDA device \n",
        "print(\"CUDA Device Name: \", torch.cuda.get_device_name(0))\n",
        "\n",
        "# Set PyTorch to use the CUDA device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "CUDA Available:  True\nCUDA Device Name:  NVIDIA A100 80GB PCIe\nUsing device: cuda\n"
        }
      ],
      "execution_count": 16,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1715088678290
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 1: Setup Environment and Install Dependencies"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from datasets import load_dataset\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration, Trainer, TrainingArguments"
      ],
      "outputs": [],
      "execution_count": 17,
      "metadata": {
        "gather": {
          "logged": 1715088680342
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 2: Load and Prepare Data"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
        "dataset = load_dataset('ccdv/cnn_dailymail', '3.0.0')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Reusing dataset cnn_dailymail (/home/azureuser/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/0107f7388b5c6fae455a5661bcd134fc22da53ea75852027040d8d1e997f101f)\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4c8bec015e214186b3910c9081943937"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1715087413653
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Randomly sample 25% of each dataset split\n",
        "dataset['train'] = dataset['train'].shuffle(seed=42).select(range(int(len(dataset['train']) * 0.25)))\n",
        "dataset['validation'] = dataset['validation'].shuffle(seed=42).select(range(int(len(dataset['validation']) * 0.25)))\n",
        "dataset['test'] = dataset['test'].shuffle(seed=42).select(range(int(len(dataset['test']) * 0.25)))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Loading cached shuffled indices for dataset at /home/azureuser/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/0107f7388b5c6fae455a5661bcd134fc22da53ea75852027040d8d1e997f101f/cache-9e4a37197c27169c.arrow\nLoading cached shuffled indices for dataset at /home/azureuser/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/0107f7388b5c6fae455a5661bcd134fc22da53ea75852027040d8d1e997f101f/cache-bc79b6d764210737.arrow\nLoading cached shuffled indices for dataset at /home/azureuser/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/0107f7388b5c6fae455a5661bcd134fc22da53ea75852027040d8d1e997f101f/cache-5e4f6217fa60443c.arrow\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1715087417978
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    # Replace multiple spaces with a single space\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "def handle_special_content(text):\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    # Optional: Remove or substitute certain other non-textual elements if present\n",
        "    return text\n",
        "\n",
        "def segment_text(tokenized_text, max_length=1024):\n",
        "    # This function assumes that the text has already been tokenized and is too long\n",
        "    return [tokenized_text[i:i + max_length] for i in range(0, len(tokenized_text), max_length)]"
      ],
      "outputs": [],
      "execution_count": 18,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1715088684947
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1715081198477
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_and_tokenize(examples):\n",
        "    # Clean and handle special content\n",
        "    cleaned_articles = [clean_text(article) for article in examples['article']]\n",
        "    cleaned_articles = [handle_special_content(article) for article in cleaned_articles]\n",
        "    \n",
        "    # Tokenize articles\n",
        "    model_inputs = tokenizer(cleaned_articles, max_length=1024, truncation=True, padding=\"max_length\")\n",
        "    \n",
        "    # Tokenize highlights\n",
        "    cleaned_highlights = [clean_text(highlight) for highlight in examples['highlights']]\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(cleaned_highlights, max_length=1024, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ],
      "outputs": [],
      "execution_count": 19,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1715088688527
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply preprocessing and tokenization to the dataset\n",
        "tokenized_datasets = dataset.map(preprocess_and_tokenize, batched=True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Parameter 'function'=<function preprocess_and_tokenize at 0x7f74ab594d30> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/72 [00:00<?, ?ba/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "41eb387331894f8dbb38562ce7a6cc4b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/4 [00:00<?, ?ba/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8325fed2cc3247408e0a793a6cecbb67"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/3 [00:00<?, ?ba/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2e9f00dcd9d74ba7a2ed9b58d6aaa237"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1715087710097
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 4: Define Evaluation Metrics"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_metric\n",
        "\n",
        "rouge = load_metric(\"rouge\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    # Decode the predictions and labels\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Sometimes you might need to further process decoded_preds and decoded_labels to remove padding or unwanted tokens\n",
        "    # Let's assume decoded_preds and decoded_labels are lists of strings (summaries)\n",
        "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
        "    decoded_labels = [[label.strip()] for label in decoded_labels]  # ROUGE expects a list of references for each prediction\n",
        "\n",
        "    # Calculate ROUGE scores\n",
        "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    # Extract a few particular scores to return\n",
        "    result = {key: value.mid.fmeasure for key, value in result.items()}  # mid.fmeasure gives the F1 score\n",
        "    return result"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1715081678477
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 5: Training the Model"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartForConditionalGeneration\n",
        "\n",
        "# Loading a distilled version of BART\n",
        "model = BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-6-6')"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1715081696379
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Clear cache\n",
        "torch.cuda.empty_cache()"
      ],
      "outputs": [],
      "execution_count": 20,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1715086657644
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Check if GPU is available and set the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Set up training arguments with mixed precision and possible gradient accumulation\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',                    # Output directory for model checkpoints\n",
        "    num_train_epochs=3,                        # Number of training epochs\n",
        "    per_device_train_batch_size=16,            # Batch size per device during training\n",
        "    per_device_eval_batch_size=16,             # Batch size for evaluation\n",
        "    warmup_steps=500,                          # Number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,                         # Weight decay for regularization\n",
        "    logging_dir='./logs',                      # Directory for storing logs\n",
        "    logging_steps=50,\n",
        "    evaluation_strategy=\"steps\",               # Evaluate at the end of each epoch\n",
        "    eval_steps=500,\n",
        "    save_steps=500,\n",
        "    save_strategy=\"steps\",                     # Save the model at the end of each epoch\n",
        "    fp16=True,                                 # Enable mixed precision training (requires NVIDIA GPU with Tensor Cores)\n",
        "    gradient_accumulation_steps=16,             # Adjust based on your GPU memory capacity and batch size\n",
        "    load_best_model_at_end=True,               # Load the best model found during training at the end\n",
        "    metric_for_best_model='eval_loss',         # Use eval loss to determine the best model\n",
        "    greater_is_better=False,                   # Lower eval loss is better\n",
        "    save_total_limit=3\n",
        ")\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets['train'],\n",
        "    eval_dataset=tokenized_datasets['validation'],\n",
        "    compute_metrics=compute_metrics  # Make sure to define compute_metrics function if you're using it\n",
        ")\n",
        "\n",
        "# Clear any cached memory to maximize available GPU memory before training starts\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Start the training process\n",
        "trainer.train()\n",
        "\n",
        "# Optionally, you can save the model manually if needed\n",
        "model.save_pretrained('./final_model_summarization')\n",
        "tokenizer.save_pretrained('./final_model_summarization')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "PyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\nUsing amp fp16 backend\nThe following columns in the training set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: article, highlights, id.\n***** Running training *****\n  Num examples = 71778\n  Num Epochs = 3\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 512\n  Gradient Accumulation steps = 16\n  Total optimization steps = 420\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='420' max='420' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [420/420 1:01:14, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nConfiguration saved in ./final_model_summarization/config.json\nModel weights saved in ./final_model_summarization/pytorch_model.bin\ntokenizer config file saved in ./final_model_summarization/tokenizer_config.json\nSpecial tokens file saved in ./final_model_summarization/special_tokens_map.json\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 16,
          "data": {
            "text/plain": "('./final_model_summarization/tokenizer_config.json',\n './final_model_summarization/special_tokens_map.json',\n './final_model_summarization/vocab.json',\n './final_model_summarization/merges.txt',\n './final_model_summarization/added_tokens.json')"
          },
          "metadata": {}
        }
      ],
      "execution_count": 16,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1715085530074
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace\n",
        "ws = Workspace.from_config()  # Or use .get() with explicit parameters\n",
        "compute_target = ws.compute_targets['bkathuri2']\n",
        "compute_target.stop(show_output=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1715065628966
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartForConditionalGeneration, BartTokenizer, Trainer, TrainingArguments\n",
        "from datasets import load_metric, load_dataset\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = BartTokenizer.from_pretrained('./final_model_summarization')\n",
        "\n",
        "# Load the model\n",
        "model = BartForConditionalGeneration.from_pretrained('./final_model_summarization')\n"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1715087357605
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_summary(text, tokenizer, model, device):\n",
        "    # Preprocess and tokenize the text\n",
        "    text = clean_text(text)\n",
        "    text = handle_special_content(text)\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=1024, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    # Move tensors to the appropriate device\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "    # Generate summary with the model\n",
        "    summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=200, early_stopping=True)\n",
        "\n",
        "    # Decode the summary\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    return summary\n",
        "\n",
        "# Example text\n",
        "example_text = \"Insert your text here that you want to summarize.\"\n",
        "\n",
        "# Generate the summary\n",
        "summary = generate_summary(example_text, tokenizer, model, device)\n",
        "print(\"Generated Summary:\", summary)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Generated Summary: The Daily Discussion is a written version of each day's featured news stories. Use this weekly Newsquiz to test your knowledge of stories you saw on CNN.com. Today's Daily Discussion includes the weekly newsquiz. Click here to read your story. Back to the page you came from.\n"
        }
      ],
      "execution_count": 20,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1715088695440
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 21,
          "data": {
            "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['article', 'highlights', 'id'],\n        num_rows: 71778\n    })\n    validation: Dataset({\n        features: ['article', 'highlights', 'id'],\n        num_rows: 3342\n    })\n    test: Dataset({\n        features: ['article', 'highlights', 'id'],\n        num_rows: 2872\n    })\n})"
          },
          "metadata": {}
        }
      ],
      "execution_count": 21,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1715088812096
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset('ccdv/cnn_dailymail', '3.0.0')\n",
        "\n",
        "def get_random_article(dataset, split='test'):\n",
        "    # Randomly pick an article from the specified split\n",
        "    random_index = random.randint(0, len(dataset[split]) - 1)\n",
        "    article = dataset[split][random_index]['article']\n",
        "    return article\n",
        "\n",
        "def generate_summary(text, tokenizer, model, device):\n",
        "    # Preprocess and tokenize the text\n",
        "    text = clean_text(text)\n",
        "    text = handle_special_content(text)\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=1024, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    # Move tensors to the appropriate device\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "    # Generate summary with the model\n",
        "    summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=200, early_stopping=True)\n",
        "\n",
        "    # Decode the summary\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    return summary\n",
        "\n",
        "# Pick a random article from the test split of the dataset\n",
        "random_article = get_random_article(dataset)\n",
        "\n",
        "# Assuming the model and device are already set up and loaded correctly\n",
        "summary = generate_summary(random_article, tokenizer, model, device)\n",
        "print(\"Generated Summary:\", summary)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "loading file https://huggingface.co/facebook/bart-large-cnn/resolve/main/vocab.json from cache at /home/azureuser/.cache/huggingface/transformers/4d8eeedc3498bc73a4b72411ebb3219209b305663632d77a6f16e60790b18038.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\nloading file https://huggingface.co/facebook/bart-large-cnn/resolve/main/merges.txt from cache at /home/azureuser/.cache/huggingface/transformers/0ddddd3ca9e107b17a6901c92543692272af1c3238a8d7549fa937ba0057bbcf.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\nloading file https://huggingface.co/facebook/bart-large-cnn/resolve/main/added_tokens.json from cache at None\nloading file https://huggingface.co/facebook/bart-large-cnn/resolve/main/special_tokens_map.json from cache at None\nloading file https://huggingface.co/facebook/bart-large-cnn/resolve/main/tokenizer_config.json from cache at None\nloading file https://huggingface.co/facebook/bart-large-cnn/resolve/main/tokenizer.json from cache at /home/azureuser/.cache/huggingface/transformers/55c96bd962ce1d360fde4947619318f1b4eb551430de678044699cbfeb99de6a.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\nloading configuration file https://huggingface.co/facebook/bart-large-cnn/resolve/main/config.json from cache at /home/azureuser/.cache/huggingface/transformers/199ab6c0f28e763098fd3ea09fd68a0928bb297d0f76b9f3375e8a1d652748f9.930264180d256e6fe8e4ba6a728dd80e969493c23d4caa0a6f943614c52d34ab\nModel config BartConfig {\n  \"_num_labels\": 3,\n  \"activation_dropout\": 0.0,\n  \"activation_function\": \"gelu\",\n  \"add_final_layer_norm\": false,\n  \"architectures\": [\n    \"BartForConditionalGeneration\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 0,\n  \"classif_dropout\": 0.0,\n  \"classifier_dropout\": 0.0,\n  \"d_model\": 1024,\n  \"decoder_attention_heads\": 16,\n  \"decoder_ffn_dim\": 4096,\n  \"decoder_layerdrop\": 0.0,\n  \"decoder_layers\": 12,\n  \"decoder_start_token_id\": 2,\n  \"dropout\": 0.1,\n  \"early_stopping\": true,\n  \"encoder_attention_heads\": 16,\n  \"encoder_ffn_dim\": 4096,\n  \"encoder_layerdrop\": 0.0,\n  \"encoder_layers\": 12,\n  \"eos_token_id\": 2,\n  \"force_bos_token_to_be_generated\": true,\n  \"forced_bos_token_id\": 0,\n  \"forced_eos_token_id\": 2,\n  \"gradient_checkpointing\": false,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\"\n  },\n  \"init_std\": 0.02,\n  \"is_encoder_decoder\": true,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2\n  },\n  \"length_penalty\": 2.0,\n  \"max_length\": 142,\n  \"max_position_embeddings\": 1024,\n  \"min_length\": 56,\n  \"model_type\": \"bart\",\n  \"no_repeat_ngram_size\": 3,\n  \"normalize_before\": false,\n  \"num_beams\": 4,\n  \"num_hidden_layers\": 12,\n  \"output_past\": true,\n  \"pad_token_id\": 1,\n  \"prefix\": \" \",\n  \"scale_embedding\": false,\n  \"task_specific_params\": {\n    \"summarization\": {\n      \"early_stopping\": true,\n      \"length_penalty\": 2.0,\n      \"max_length\": 142,\n      \"min_length\": 56,\n      \"no_repeat_ngram_size\": 3,\n      \"num_beams\": 4\n    }\n  },\n  \"transformers_version\": \"4.12.2\",\n  \"use_cache\": true,\n  \"vocab_size\": 50264\n}\n\nReusing dataset cnn_dailymail (/home/azureuser/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/0107f7388b5c6fae455a5661bcd134fc22da53ea75852027040d8d1e997f101f)\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ba3c6d91044645ef9e2ec3117b921228"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Generated Summary: Jenson Button failed to complete qualifying session for the Bahrain Grand Prix. The British driver was set to start from the back of the grid. McLaren chairman Ron Dennis said the engine is not broken. Button took to Twitter to update his fans following yet another car failure. He then gave his verdict as the race unfolded tweeting live updates.\n"
        }
      ],
      "execution_count": 22,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1715088967919
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "nlp",
      "language": "python",
      "display_name": "nlp"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.19",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "nlp"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}